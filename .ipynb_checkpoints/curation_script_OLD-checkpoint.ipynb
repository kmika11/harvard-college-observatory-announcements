{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curation Script - Chinese Maritime Trade Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "- This script contains the basic curation workflow for creating datasets and uploading files from a metadata spreadsheet of data files.   \n",
    "- **Be SURE `curate.py` and `directupload.py` are set in correct directories**\n",
    "<p>\n",
    "- **Created:** 2023/10/12\n",
    "- **Updated:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals\n",
    "- Global variables for this script. \n",
    "- Set variable names (e.g., `g_api_key` as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set curation source path\n",
    "g_module_path = '/Users/katherinemika/Desktop/curation/historic_datasets/shanghai_returns/1936'\n",
    "\n",
    "# path to output file\n",
    "g_dataverse_inventory_file = '/Users/katherinemika/Desktop/curation/historic_datasets/shanghai_returns/1936/1936_shanghai_returns_metadata.csv'\n",
    "# series names\n",
    "g_series_names = []\n",
    "\n",
    "# dataset inventories (keyed on series name)\n",
    "g_series_inventories = {}\n",
    "\n",
    "# dataset metadata (keyed on series name)\n",
    "g_dataset_metadata = {}\n",
    "\n",
    "# dataverse installation\n",
    "g_dataverse_installation_url = 'https://dataverse.harvard.edu'\n",
    "\n",
    "# dataverse API key\n",
    "g_dataverse_api_key = 'xxx-xxxxx-xxxxx-xxxxx-xxx'\n",
    "\n",
    "# dataverse collection name\n",
    "g_dataverse_collection = 'shanghai_returns'\n",
    "\n",
    "# dataverse inventory dataframe\n",
    "g_dataverse_inventory_df = None\n",
    "\n",
    "# dataset author\n",
    "g_dataset_author = 'Mika, Katherine'\n",
    "\n",
    "# dataset author affiliation\n",
    "g_dataset_author_affiliation = 'Harvard Library'\n",
    "\n",
    "# dataset contact information\n",
    "g_dataset_contact = 'Mika, Katherine'\n",
    "g_dataset_contact_email = 'katherine_mika@harvard.edu'\n",
    "\n",
    "# full path to location of datafiles (e.g., ../data/trade_statistics)\n",
    "g_datafiles_path = '/Users/katherinemika/Desktop/curation/historic_datasets/shanghai_returns/1936/csv_and_txt'\n",
    "# demo dataverse dataset information (keyed on series name)\n",
    "g_dataverse_dataset_info = {}\n",
    "\n",
    "# datafile metadata (dataframe of datafile metadata, keyed on series name)\n",
    "g_datafile_metadata = {}\n",
    "\n",
    "# datafile description template\n",
    "g_datafile_description_template_txt = 'File contains OCR text with data from Port: '\n",
    "g_datafile_description_template_csv = 'File contains csv table with data from Port: '\n",
    "\n",
    "\n",
    "# dataset batches (array of batches of series to create/upload)\n",
    "g_dataset_batches = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add local modules path to Jupyter system path\n",
    "- Load all modules including local modules such as `curate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if g_module_path not in sys.path:\n",
    "    sys.path.append(g_module_path)\n",
    "\n",
    "import curate\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint as pprint\n",
    "from pyDataverse.api import NativeApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dictionary of dataset pids keyed on series name. Collected from output of \"create dataset\" function\n",
    "def get_dataset_pids(batch, dataset_info):\n",
    "    pids = {}\n",
    "    for series_name in batch:\n",
    "        pids[series_name] = dataset_info[series_name].get('dataset_pid')\n",
    "    return pids\n",
    "\n",
    "def get_dataset_pids_from_hdv(api, dataverse_collection):\n",
    "    ds_tree = api.get_children(dataverse_collection, children_types=[\"datasets\"])\n",
    "    key = api.api_token\n",
    "    base_url = api.base_url\n",
    "    headers = {'X-Dataverse-key': key}\n",
    "    \n",
    "    formatted_ds_tree = {}\n",
    "    \n",
    "    for t in ds_tree:\n",
    "        pid = t['pid']\n",
    "        request_url = '{}/api/datasets/:persistentId/?persistentId={}'.format(base_url, pid)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            r = response.json()\n",
    "            title = r['data']['latestVersion']['metadataBlocks']['citation']['fields'][0]['value']\n",
    "            dataset_id = r['data']['id']\n",
    "            formatted_ds_tree[title] = {\n",
    "                'dataset_id': dataset_id,\n",
    "                'dataset_pid': pid,\n",
    "                'status': True\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for PID: {pid}\")\n",
    "    \n",
    "    return formatted_ds_tree\n",
    "\n",
    "# get dictionary of datafile inventories keyed on series name\n",
    "def get_datafile_inventories(batch, datafile_metadata):\n",
    "    inventories = {}\n",
    "    for series_name in batch:\n",
    "        inventories[series_name] = datafile_metadata[series_name]\n",
    "    return inventories \n",
    "\n",
    "# upload the datafiles associated with a batch\n",
    "def upload_dataset_batch(api, dataverse_url, batch_list, batch_pids, batch_datafile_metadata, data_directory):\n",
    "    # upload the datafiles associated with each series in the batch\n",
    "    results = {}\n",
    "    for series_name in batch_list:\n",
    "        pid = batch_pids[series_name]\n",
    "        datafiles_metadata = batch_datafile_metadata[series_name]\n",
    "        results[series_name] = curate.direct_upload_datafiles(api, dataverse_url, pid, path, datafiles_metadata)\n",
    "    return results\n",
    "\n",
    "# upload datafiles for a batch of datasets\n",
    "def upload_files_batch(api, dataverse_url, batch_list, batch_pids, data_directory, batch_datafile_metadata):\n",
    "    results = {}\n",
    "    for series_name in batch_list:\n",
    "        pid = batch_pids[series_name]\n",
    "        batch_datafile_metadata = g_datafile_metadata[series_name]\n",
    "        results[series_name] = curate.python_dvuploader(api, dataverse_url, pid, data_directory, batch_datafile_metadata)\n",
    "    return results\n",
    "\n",
    "# upload datafiles for a batch of datasets - but ignoring 500 errors that the python_dvuploader keeps issuing...\n",
    "def upload_files_batch_errors(api, dataverse_url, batch_list, batch_pids, data_directory, g_datafile_metadata):\n",
    "    results = {}\n",
    "    for series_name in batch_list:\n",
    "        try:\n",
    "            pid = batch_pids[series_name]\n",
    "            batch_datafile_metadata = g_datafile_metadata[series_name]\n",
    "            results[series_name] = curate.python_dvuploader(api, dataverse_url, pid, data_directory, batch_datafile_metadata)\n",
    "        except Exception as e:\n",
    "            if \"500\" in str(e):\n",
    "                print(f\"500 error encountered for {series_name}. Skipping to the next series.\")\n",
    "                continue\n",
    "            else:\n",
    "                # Handle other exceptions if necessary\n",
    "                raise e\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate Inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare inventory data for curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Read `dataverse_inventory`\n",
    "- **Add instructions about spreadsheet format** Eg. Column names, table org, etc.\n",
    "- Create a `DataFrame` for later use\n",
    "- Note: Also, the `curate:direct_upload_datafiles` function expects all files to be in a single directory (not grouped by file type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: UTF-8-SIG\n"
     ]
    }
   ],
   "source": [
    "# read the dataverse inventory file\n",
    "import chardet\n",
    "\n",
    "with open(g_dataverse_inventory_file, 'rb') as f: \n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"detected encoding: {encoding}\")\n",
    "    \n",
    "g_dataverse_inventory_df = pd.read_csv(g_dataverse_inventory_file,index_col=None, encoding=encoding, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Analysis of Imports\n",
       "1      Analysis of Imports\n",
       "2      Analysis of Imports\n",
       "3      Analysis of Imports\n",
       "4      Analysis of Imports\n",
       "              ...         \n",
       "251    Analysis of Exports\n",
       "252           By Countries\n",
       "253           By Countries\n",
       "254           By Countries\n",
       "255           By Countries\n",
       "Name: series_name, Length: 256, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_dataverse_inventory_df['series_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Create Dataset Inventories\n",
    "- Get the list of series names\n",
    "- Create a `dict` of file inventories keyed on series name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis of Imports', 'Analysis of Exports', 'By Countries']\n"
     ]
    }
   ],
   "source": [
    "# get list of series in the full inventory\n",
    "g_series_names = list(g_dataverse_inventory_df.series_name.unique())\n",
    "\n",
    "# create series inventories\n",
    "for name in g_series_names:\n",
    "    # get series inventory\n",
    "    g_series_inventories[name] = g_dataverse_inventory_df.loc[g_dataverse_inventory_df['series_name'] == name]\n",
    "\n",
    "pprint.pprint(g_series_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Create Dataset Metadata\n",
    "- Create a `dict` of dataset metadata extracted from each inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Analysis of Exports': {'author': [{'authorAffiliation': 'Harvard Library',\n",
      "                                     'authorName': 'Mika, Katherine'}],\n",
      "                         'contact': [{'datasetContactAffiliation': 'Harvard '\n",
      "                                                                   'Library',\n",
      "                                      'datasetContactEmail': 'katherine_mika@harvard.edu',\n",
      "                                      'datasetContactName': 'Mika, Katherine'}],\n",
      "                         'creation_date': '1936-01-01',\n",
      "                         'data_source': ['http://nrs.harvard.edu/urn-3:FHCL:9446183'],\n",
      "                         'description': [{'dsDescriptionValue': 'Analysis of '\n",
      "                                                                'Exports is a '\n",
      "                                                                'series of '\n",
      "                                                                'tables and '\n",
      "                                                                'text files '\n",
      "                                                                'associated '\n",
      "                                                                'with: '\n",
      "                                                                'Shanghai '\n",
      "                                                                'annual '\n",
      "                                                                'returns of '\n",
      "                                                                'foreign '\n",
      "                                                                'trade, 1936 : '\n",
      "                                                                'analysis of '\n",
      "                                                                'imports and '\n",
      "                                                                'exports. '\n",
      "                                                                'Created by: '\n",
      "                                                                'China. Hai '\n",
      "                                                                'guan zong '\n",
      "                                                                'shui wu si '\n",
      "                                                                'shu.\\xa0'}],\n",
      "                         'keywords': [{'keywordValue': 'China -- Commerce -- '\n",
      "                                                       'Statistics -- '\n",
      "                                                       'Periodicals',\n",
      "                                       'keywordVocabulary': 'LCSH',\n",
      "                                       'keywordVocabularyURI': 'https://www.loc.gov/aba/cataloging/subject/'},\n",
      "                                      {'keywordValue': ' China -- Commerce -- '\n",
      "                                                       'Periodicals',\n",
      "                                       'keywordVocabulary': 'LCSH',\n",
      "                                       'keywordVocabularyURI': 'https://www.loc.gov/aba/cataloging/subject/'}],\n",
      "                         'license': 'CC0 1.0',\n",
      "                         'origin_of_sources': '<a '\n",
      "                                              'href=\"https://id.lib.harvard.edu/alma/990058255550203941/catalog\">Shanghai '\n",
      "                                              'annual returns of foreign '\n",
      "                                              'trade, 1936 : analysis of '\n",
      "                                              'imports and exports</a>',\n",
      "                         'subject': ['Social Sciences'],\n",
      "                         'title': 'Analysis of Exports',\n",
      "                         'topic_classification': [{'topicClassValue': 'Economic '\n",
      "                                                                      'History'}]},\n",
      " 'Analysis of Imports': {'author': [{'authorAffiliation': 'Harvard Library',\n",
      "                                     'authorName': 'Mika, Katherine'}],\n",
      "                         'contact': [{'datasetContactAffiliation': 'Harvard '\n",
      "                                                                   'Library',\n",
      "                                      'datasetContactEmail': 'katherine_mika@harvard.edu',\n",
      "                                      'datasetContactName': 'Mika, Katherine'}],\n",
      "                         'creation_date': '1936-01-01',\n",
      "                         'data_source': ['http://nrs.harvard.edu/urn-3:FHCL:9446183'],\n",
      "                         'description': [{'dsDescriptionValue': 'Analysis of '\n",
      "                                                                'Imports is a '\n",
      "                                                                'series of '\n",
      "                                                                'tables and '\n",
      "                                                                'text files '\n",
      "                                                                'associated '\n",
      "                                                                'with: '\n",
      "                                                                'Shanghai '\n",
      "                                                                'annual '\n",
      "                                                                'returns of '\n",
      "                                                                'foreign '\n",
      "                                                                'trade, 1936 : '\n",
      "                                                                'analysis of '\n",
      "                                                                'imports and '\n",
      "                                                                'exports. '\n",
      "                                                                'Created by: '\n",
      "                                                                'China. Hai '\n",
      "                                                                'guan zong '\n",
      "                                                                'shui wu si '\n",
      "                                                                'shu.\\xa0'}],\n",
      "                         'keywords': [{'keywordValue': 'China -- Commerce -- '\n",
      "                                                       'Statistics -- '\n",
      "                                                       'Periodicals',\n",
      "                                       'keywordVocabulary': 'LCSH',\n",
      "                                       'keywordVocabularyURI': 'https://www.loc.gov/aba/cataloging/subject/'},\n",
      "                                      {'keywordValue': ' China -- Commerce -- '\n",
      "                                                       'Periodicals',\n",
      "                                       'keywordVocabulary': 'LCSH',\n",
      "                                       'keywordVocabularyURI': 'https://www.loc.gov/aba/cataloging/subject/'}],\n",
      "                         'license': 'CC0 1.0',\n",
      "                         'origin_of_sources': '<a '\n",
      "                                              'href=\"https://id.lib.harvard.edu/alma/990058255550203941/catalog\">Shanghai '\n",
      "                                              'annual returns of foreign '\n",
      "                                              'trade, 1936 : analysis of '\n",
      "                                              'imports and exports</a>',\n",
      "                         'subject': ['Social Sciences'],\n",
      "                         'title': 'Analysis of Imports',\n",
      "                         'topic_classification': [{'topicClassValue': 'Economic '\n",
      "                                                                      'History'}]},\n",
      " 'By Countries': {'author': [{'authorAffiliation': 'Harvard Library',\n",
      "                              'authorName': 'Mika, Katherine'}],\n",
      "                  'contact': [{'datasetContactAffiliation': 'Harvard Library',\n",
      "                               'datasetContactEmail': 'katherine_mika@harvard.edu',\n",
      "                               'datasetContactName': 'Mika, Katherine'}],\n",
      "                  'creation_date': '1936-01-01',\n",
      "                  'data_source': ['http://nrs.harvard.edu/urn-3:FHCL:9446183'],\n",
      "                  'description': [{'dsDescriptionValue': 'By Countries is a '\n",
      "                                                         'series of tables and '\n",
      "                                                         'text files '\n",
      "                                                         'associated with: '\n",
      "                                                         'Shanghai annual '\n",
      "                                                         'returns of foreign '\n",
      "                                                         'trade, 1936 : '\n",
      "                                                         'analysis of imports '\n",
      "                                                         'and exports. Created '\n",
      "                                                         'by: China. Hai guan '\n",
      "                                                         'zong shui wu si '\n",
      "                                                         'shu.\\xa0'}],\n",
      "                  'keywords': [{'keywordValue': 'China -- Commerce -- '\n",
      "                                                'Statistics -- Periodicals',\n",
      "                                'keywordVocabulary': 'LCSH',\n",
      "                                'keywordVocabularyURI': 'https://www.loc.gov/aba/cataloging/subject/'},\n",
      "                               {'keywordValue': ' China -- Commerce -- '\n",
      "                                                'Periodicals',\n",
      "                                'keywordVocabulary': 'LCSH',\n",
      "                                'keywordVocabularyURI': 'https://www.loc.gov/aba/cataloging/subject/'}],\n",
      "                  'license': 'CC0 1.0',\n",
      "                  'origin_of_sources': '<a '\n",
      "                                       'href=\"https://id.lib.harvard.edu/alma/990058255550203941/catalog\">Shanghai '\n",
      "                                       'annual returns of foreign trade, 1936 '\n",
      "                                       ': analysis of imports and exports</a>',\n",
      "                  'subject': ['Social Sciences'],\n",
      "                  'title': 'By Countries',\n",
      "                  'topic_classification': [{'topicClassValue': 'Economic '\n",
      "                                                               'History'}]}}\n"
     ]
    }
   ],
   "source": [
    "# for each series name, create dataset metadata\n",
    "for series_name in g_series_names:\n",
    "    # get series inventory\n",
    "    series_inventory = g_series_inventories[series_name]\n",
    "    md = curate.create_dataset_metadata(g_dataset_author, g_dataset_author_affiliation, \n",
    "                                        g_dataset_contact, g_dataset_contact_email,\n",
    "                                        series_name, series_inventory)\n",
    "    g_dataset_metadata[series_name] = md\n",
    "\n",
    "pprint.pprint(g_dataset_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create Datafile Metadata\n",
    "- Create a `dict` of `DataFrames` containing metadata about individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for series_name in g_series_names:\n",
    "    # get dataset metadata for the series\n",
    "    series_metadata = g_dataset_metadata[series_name]\n",
    "    # get the series inventory\n",
    "    series_inventory_df = g_series_inventories[series_name]\n",
    "    # create datafile metadata\n",
    "    g_datafile_metadata[series_name] = curate.create_datafile_metadata(series_inventory_df, g_datafile_description_template_csv, g_datafile_description_template_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename_osn</th>\n",
       "      <th>custom_name</th>\n",
       "      <th>file_type</th>\n",
       "      <th>description</th>\n",
       "      <th>mimetype</th>\n",
       "      <th>tags</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44421113.txt</td>\n",
       "      <td>Imports_44421113.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>File contains OCR text with data from Port:  A...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>[\"Data\", \"Shanghai\", \" Limited\", \" HARVARD\", \"...</td>\n",
       "      <td>44421113.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44421114.txt</td>\n",
       "      <td>Imports_44421114.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>File contains OCR text with data from Port:  A...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>[\"Data\", \"HARVARD\", \" LIBRARY\", \" ARTHUR\", \" O...</td>\n",
       "      <td>44421114.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44421115.txt</td>\n",
       "      <td>Imports_44421115.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>File contains OCR text with data from Port:  A...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>[\"Data\", \"Shanghai\", \" Limited\", \" SERIES\", \" ...</td>\n",
       "      <td>44421115.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44421116.txt</td>\n",
       "      <td>Imports_44421116.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>File contains OCR text with data from Port:  A...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>[\"Data\", \"HARVARD\", \" HARVARD UNIVERSITY\", \" J...</td>\n",
       "      <td>44421116.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44421117.txt</td>\n",
       "      <td>Imports_44421117.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>File contains OCR text with data from Port:  A...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>[\"Data\", \"Chinese\", \" Weights\", \" Measures, \"]</td>\n",
       "      <td>44421117.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>44421418.txt</td>\n",
       "      <td>Imports_44421418.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>File contains OCR text with data from Port:  A...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>[\"Data\"]</td>\n",
       "      <td>44421418.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>44421420.txt</td>\n",
       "      <td>Imports_44421420.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>File contains OCR text with data from Port:  A...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>[\"Data\"]</td>\n",
       "      <td>44421420.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>44421423.txt</td>\n",
       "      <td>Imports_44421423.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>File contains OCR text with data from Port:  A...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>[\"Data\"]</td>\n",
       "      <td>44421423.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>44421425.txt</td>\n",
       "      <td>Imports_44421425.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>File contains OCR text with data from Port:  A...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>[\"Data\"]</td>\n",
       "      <td>44421425.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>44421425_301-331.csv</td>\n",
       "      <td>Imports_44421425_301-331.csv</td>\n",
       "      <td>csv</td>\n",
       "      <td>File contains csv table with data from Port:  ...</td>\n",
       "      <td>text/csv</td>\n",
       "      <td>[\"Data\", \"Dyed\", \" Yarn\", \" White or Dyed\", \" ...</td>\n",
       "      <td>44421425_301-331.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename_osn                   custom_name file_type  \\\n",
       "0            44421113.txt          Imports_44421113.txt       txt   \n",
       "1            44421114.txt          Imports_44421114.txt       txt   \n",
       "2            44421115.txt          Imports_44421115.txt       txt   \n",
       "3            44421116.txt          Imports_44421116.txt       txt   \n",
       "4            44421117.txt          Imports_44421117.txt       txt   \n",
       "..                    ...                           ...       ...   \n",
       "165          44421418.txt          Imports_44421418.txt       txt   \n",
       "166          44421420.txt          Imports_44421420.txt       txt   \n",
       "167          44421423.txt          Imports_44421423.txt       txt   \n",
       "168          44421425.txt          Imports_44421425.txt       txt   \n",
       "169  44421425_301-331.csv  Imports_44421425_301-331.csv       csv   \n",
       "\n",
       "                                           description    mimetype  \\\n",
       "0    File contains OCR text with data from Port:  A...  text/plain   \n",
       "1    File contains OCR text with data from Port:  A...  text/plain   \n",
       "2    File contains OCR text with data from Port:  A...  text/plain   \n",
       "3    File contains OCR text with data from Port:  A...  text/plain   \n",
       "4    File contains OCR text with data from Port:  A...  text/plain   \n",
       "..                                                 ...         ...   \n",
       "165  File contains OCR text with data from Port:  A...  text/plain   \n",
       "166  File contains OCR text with data from Port:  A...  text/plain   \n",
       "167  File contains OCR text with data from Port:  A...  text/plain   \n",
       "168  File contains OCR text with data from Port:  A...  text/plain   \n",
       "169  File contains csv table with data from Port:  ...    text/csv   \n",
       "\n",
       "                                                  tags                  test  \n",
       "0    [\"Data\", \"Shanghai\", \" Limited\", \" HARVARD\", \"...          44421113.txt  \n",
       "1    [\"Data\", \"HARVARD\", \" LIBRARY\", \" ARTHUR\", \" O...          44421114.txt  \n",
       "2    [\"Data\", \"Shanghai\", \" Limited\", \" SERIES\", \" ...          44421115.txt  \n",
       "3    [\"Data\", \"HARVARD\", \" HARVARD UNIVERSITY\", \" J...          44421116.txt  \n",
       "4       [\"Data\", \"Chinese\", \" Weights\", \" Measures, \"]          44421117.txt  \n",
       "..                                                 ...                   ...  \n",
       "165                                           [\"Data\"]          44421418.txt  \n",
       "166                                           [\"Data\"]          44421420.txt  \n",
       "167                                           [\"Data\"]          44421423.txt  \n",
       "168                                           [\"Data\"]          44421425.txt  \n",
       "169  [\"Data\", \"Dyed\", \" Yarn\", \" White or Dyed\", \" ...  44421425_301-331.csv  \n",
       "\n",
       "[170 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_datafile_metadata['Analysis of Imports']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create Series Batches\n",
    "- Create a set of (approximately) equal length batches of series (to create dataset and upload datafiles)\n",
    "- Generally, there are too many series in a volume to create the related datasets and then upload all their datafiles in a single tight loop. Therefore, it's useful to create batches of these series and perform the create/upload operation on a single batch at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['Analysis of Imports', 'Analysis of Exports', 'By Countries'],\n",
      "      dtype='<U19')]\n"
     ]
    }
   ],
   "source": [
    "# max number of series in a batch\n",
    "batch_size = 3\n",
    "g_batches = np.array_split(g_series_names, len(g_series_names)/batch_size)\n",
    "pprint.pprint(g_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize `pyDataverse` API\n",
    "- Use `pyDataverse` to initialize the API to the dataverse installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native API: https://dataverse.harvard.edu/api/v1\n"
     ]
    }
   ],
   "source": [
    "# set pyDataverse API adapter\n",
    "g_api = NativeApi(g_dataverse_installation_url, g_dataverse_api_key)\n",
    "\n",
    "# print results\n",
    "print('{}'.format(g_api))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Datasets and Upload Datafiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Create all datasets\n",
    "- For each series name, create a dataset and retain status information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Analysis of Exports': {'dataset_id': 10391014,\n",
      "                         'dataset_pid': 'doi:10.7910/DVN/HYUX15',\n",
      "                         'status': True},\n",
      " 'Analysis of Imports': {'dataset_id': 10391013,\n",
      "                         'dataset_pid': 'doi:10.7910/DVN/WIZ6ZM',\n",
      "                         'status': True},\n",
      " 'By Countries': {'dataset_id': 10391015,\n",
      "                  'dataset_pid': 'doi:10.7910/DVN/HIHES0',\n",
      "                  'status': True}}\n"
     ]
    }
   ],
   "source": [
    "# for each series, create a dataset and save its information\n",
    "for series_name in g_series_names:\n",
    "    # get the series metadata\n",
    "    series_metadata = g_dataset_metadata[series_name]\n",
    "    # create the dataset\n",
    "    g_dataverse_dataset_info[series_name] = curate.create_dataset(g_api, g_dataverse_collection, series_metadata)\n",
    "\n",
    "pprint.pprint(g_dataverse_dataset_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Get dataset PIDs from HDV collection\n",
    "\n",
    "For use if datasets have been previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only necessary if you made changes to the curate module before this section\n",
    "import importlib\n",
    "importlib.reload(curate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dataverse_dataset_info = get_dataset_pids_from_hdv(g_api, g_dataverse_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Analysis of Exports': {'dataset_id': 10391014,\n",
      "                         'dataset_pid': 'doi:10.7910/DVN/HYUX15',\n",
      "                         'status': True},\n",
      " 'Analysis of Imports': {'dataset_id': 10391013,\n",
      "                         'dataset_pid': 'doi:10.7910/DVN/WIZ6ZM',\n",
      "                         'status': True},\n",
      " 'By Countries': {'dataset_id': 10391015,\n",
      "                  'dataset_pid': 'doi:10.7910/DVN/HIHES0',\n",
      "                  'status': True}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(g_dataverse_dataset_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Upload dataset datafiles, one batch at a time\n",
    "- Upload the datafiles associated with each dataset in a batch\n",
    "- *MAKE SURE DIRECT UPLOAD IS ENABLED*\n",
    "- problem with \"registering files\" in dvuploader python library. Usually throws 500 error, but works with small number of files (eg. 1925 Kiaochow). Indexing issue?\n",
    "- Try looping batches with exception for 500 error (add in pause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading batch: 0, series: ['Analysis of Imports' 'Analysis of Exports' 'By Countries']\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────── <span style=\"font-weight: bold\">DVUploader</span> ──────────────╮\n",
       "│ Server: <span style=\"font-weight: bold\">https://dataverse.harvard.edu</span> │\n",
       "│ PID: <span style=\"font-weight: bold\">doi:10.7910/DVN/WIZ6ZM</span>           │\n",
       "│ Files: 170                            │\n",
       "╰───────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭───────────── \u001b[1mDVUploader\u001b[0m ──────────────╮\n",
       "│ Server: \u001b[1mhttps://dataverse.harvard.edu\u001b[0m │\n",
       "│ PID: \u001b[1mdoi:10.7910/DVN/WIZ6ZM\u001b[0m           │\n",
       "│ Files: 170                            │\n",
       "╰───────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84631dd11f1649ce95047f7315e5796e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">   </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold; font-style: italic\">🔎 Checking</span><span style=\"font-style: italic\">   </span>\n",
       "<span style=\"font-style: italic\">  </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold; font-style: italic\">dataset files</span><span style=\"font-style: italic\">  </span>\n",
       "┏━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> New </span>┃<span style=\"font-weight: bold\"> Skipped </span>┃\n",
       "┡━━━━━╇━━━━━━━━━┩\n",
       "│<span style=\"color: #00d75f; text-decoration-color: #00d75f\"> 170 </span>│<span style=\"color: #808080; text-decoration-color: #808080\"> 0       </span>│\n",
       "└─────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m   \u001b[0m\u001b[1;3;37m🔎 Checking\u001b[0m\u001b[3m   \u001b[0m\n",
       "\u001b[3m  \u001b[0m\u001b[1;3;37mdataset files\u001b[0m\u001b[3m  \u001b[0m\n",
       "┏━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mNew\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSkipped\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━╇━━━━━━━━━┩\n",
       "│\u001b[38;5;41m \u001b[0m\u001b[38;5;41m170\u001b[0m\u001b[38;5;41m \u001b[0m│\u001b[90m \u001b[0m\u001b[90m0      \u001b[0m\u001b[90m \u001b[0m│\n",
       "└─────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold; font-style: italic\">🚀 Uploading files</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;3;37m🚀 Uploading files\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4cc1d8be5e45dfb0c460befe5816f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 error encountered for Analysis of Imports. Skipping to the next series.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────── <span style=\"font-weight: bold\">DVUploader</span> ──────────────╮\n",
       "│ Server: <span style=\"font-weight: bold\">https://dataverse.harvard.edu</span> │\n",
       "│ PID: <span style=\"font-weight: bold\">doi:10.7910/DVN/HYUX15</span>           │\n",
       "│ Files: 82                             │\n",
       "╰───────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭───────────── \u001b[1mDVUploader\u001b[0m ──────────────╮\n",
       "│ Server: \u001b[1mhttps://dataverse.harvard.edu\u001b[0m │\n",
       "│ PID: \u001b[1mdoi:10.7910/DVN/HYUX15\u001b[0m           │\n",
       "│ Files: 82                             │\n",
       "╰───────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b59d79854a400d9d9ef2f2b906c506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">   </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold; font-style: italic\">🔎 Checking</span><span style=\"font-style: italic\">   </span>\n",
       "<span style=\"font-style: italic\">  </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold; font-style: italic\">dataset files</span><span style=\"font-style: italic\">  </span>\n",
       "┏━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> New </span>┃<span style=\"font-weight: bold\"> Skipped </span>┃\n",
       "┡━━━━━╇━━━━━━━━━┩\n",
       "│<span style=\"color: #00d75f; text-decoration-color: #00d75f\"> 82  </span>│<span style=\"color: #808080; text-decoration-color: #808080\"> 0       </span>│\n",
       "└─────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m   \u001b[0m\u001b[1;3;37m🔎 Checking\u001b[0m\u001b[3m   \u001b[0m\n",
       "\u001b[3m  \u001b[0m\u001b[1;3;37mdataset files\u001b[0m\u001b[3m  \u001b[0m\n",
       "┏━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mNew\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSkipped\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━╇━━━━━━━━━┩\n",
       "│\u001b[38;5;41m \u001b[0m\u001b[38;5;41m82 \u001b[0m\u001b[38;5;41m \u001b[0m│\u001b[90m \u001b[0m\u001b[90m0      \u001b[0m\u001b[90m \u001b[0m│\n",
       "└─────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold; font-style: italic\">🚀 Uploading files</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;3;37m🚀 Uploading files\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b588203a2d4ca6aa1f9ee978a61c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 error encountered for Analysis of Exports. Skipping to the next series.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────── <span style=\"font-weight: bold\">DVUploader</span> ──────────────╮\n",
       "│ Server: <span style=\"font-weight: bold\">https://dataverse.harvard.edu</span> │\n",
       "│ PID: <span style=\"font-weight: bold\">doi:10.7910/DVN/HIHES0</span>           │\n",
       "│ Files: 4                              │\n",
       "╰───────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭───────────── \u001b[1mDVUploader\u001b[0m ──────────────╮\n",
       "│ Server: \u001b[1mhttps://dataverse.harvard.edu\u001b[0m │\n",
       "│ PID: \u001b[1mdoi:10.7910/DVN/HIHES0\u001b[0m           │\n",
       "│ Files: 4                              │\n",
       "╰───────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe7ef2c0b204319a02f941ddd13c336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold; font-style: italic\">🔎 Checking dataset files</span><span style=\"font-style: italic\">                           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> File                           </span>┃<span style=\"font-weight: bold\"> Status </span>┃<span style=\"font-weight: bold\"> Action </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Countries_44421629.txt         </span>│ <span style=\"color: #00d75f; text-decoration-color: #00d75f\">New</span>    │ <span style=\"color: #00d75f; text-decoration-color: #00d75f\">Upload</span> │\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Countries_44421629_503-504.csv </span>│ <span style=\"color: #00d75f; text-decoration-color: #00d75f\">New</span>    │ <span style=\"color: #00d75f; text-decoration-color: #00d75f\">Upload</span> │\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Countries_44421630.txt         </span>│ <span style=\"color: #00d75f; text-decoration-color: #00d75f\">New</span>    │ <span style=\"color: #00d75f; text-decoration-color: #00d75f\">Upload</span> │\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Countries_44421632.txt         </span>│ <span style=\"color: #00d75f; text-decoration-color: #00d75f\">New</span>    │ <span style=\"color: #00d75f; text-decoration-color: #00d75f\">Upload</span> │\n",
       "└────────────────────────────────┴────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3;37m🔎 Checking dataset files\u001b[0m\u001b[3m                           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mFile                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAction\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mCountries_44421629.txt        \u001b[0m\u001b[36m \u001b[0m│ \u001b[38;5;41mNew\u001b[0m    │ \u001b[38;5;41mUpload\u001b[0m │\n",
       "│\u001b[36m \u001b[0m\u001b[36mCountries_44421629_503-504.csv\u001b[0m\u001b[36m \u001b[0m│ \u001b[38;5;41mNew\u001b[0m    │ \u001b[38;5;41mUpload\u001b[0m │\n",
       "│\u001b[36m \u001b[0m\u001b[36mCountries_44421630.txt        \u001b[0m\u001b[36m \u001b[0m│ \u001b[38;5;41mNew\u001b[0m    │ \u001b[38;5;41mUpload\u001b[0m │\n",
       "│\u001b[36m \u001b[0m\u001b[36mCountries_44421632.txt        \u001b[0m\u001b[36m \u001b[0m│ \u001b[38;5;41mNew\u001b[0m    │ \u001b[38;5;41mUpload\u001b[0m │\n",
       "└────────────────────────────────┴────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold; font-style: italic\">🚀 Uploading files</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;3;37m🚀 Uploading files\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bd5fd23f1c4e098fa1658fff39ad23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold; font-style: italic\">✅ Upload complete</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;3;37m✅ Upload complete\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "ret = upload_files_batch_errors(g_api, g_dataverse_installation_url, batch, pids, g_datafiles_path, datafile_metadata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 42\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "upload_files_batch(g_api, g_dataverse_installation_url, batch, pids, g_datafiles_path, datafile_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "import importlib\n",
    "importlib.reload(curate)\n",
    "\n",
    "# publish the datasets\n",
    "errors = curate.publish_datasets(g_api, g_dataverse_collection, version='major')\n",
    "\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inventories\n",
    "\n",
    "combine metadata files into single, long dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/Users/katherinemika/Desktop/curation/historic_datasets/\"\n",
    "annual_trade_reports_1923 = base_dir + 'annual_trade_reports/1923/1923_metadata.csv'\n",
    "annual_trade_reports_1924 = base_dir + 'annual_trade_reports/1924/1924_metadata.csv'\n",
    "annual_trade_reports_1925 = base_dir + 'annual_trade_reports/1925/1925_metadata.csv'\n",
    "annual_trade_reports_1926 = base_dir + 'annual_trade_reports/1926/1926_metadata.csv'\n",
    "annual_trade_reports_1927 = base_dir + 'annual_trade_reports/1927/1927_metadata.csv'\n",
    "annual_trade_reports_1928 = base_dir + 'annual_trade_reports/1928/1928_metadata.csv'\n",
    "returns_trade_ports = base_dir + 'returns_trade_ports/1866/1866_returns_trades_ports_metadata.csv'\n",
    "shanghai_returns = base_dir + 'shanghai_returns/1936/1936_shanghai_returns_metadata.csv'\n",
    "trade_statistics_treaty_ports = base_dir + 'trade_statistics_treaty_ports/1873/1873_metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: UTF-8-SIG\n"
     ]
    }
   ],
   "source": [
    "with open(annual_trade_reports_1923, 'rb') as f: \n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"detected encoding: {encoding}\")\n",
    "    \n",
    "df_annual_trade_reports_1923 = pd.read_csv(annual_trade_reports_1923,index_col=None, encoding=encoding, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8\n"
     ]
    }
   ],
   "source": [
    "with open(annual_trade_reports_1924, 'rb') as f: \n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"detected encoding: {encoding}\")\n",
    "    \n",
    "df_annual_trade_reports_1924 = pd.read_csv(annual_trade_reports_1924,index_col=None, encoding=encoding, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8\n"
     ]
    }
   ],
   "source": [
    "with open(annual_trade_reports_1925, 'rb') as f: \n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"detected encoding: {encoding}\")\n",
    "    \n",
    "df_annual_trade_reports_1925 = pd.read_csv(annual_trade_reports_1925,index_col=None, encoding=encoding, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8\n"
     ]
    }
   ],
   "source": [
    "with open(annual_trade_reports_1926, 'rb') as f: \n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"detected encoding: {encoding}\")\n",
    "    \n",
    "df_annual_trade_reports_1926 = pd.read_csv(annual_trade_reports_1926,index_col=None, encoding=encoding, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8\n"
     ]
    }
   ],
   "source": [
    "with open(annual_trade_reports_1927, 'rb') as f: \n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"detected encoding: {encoding}\")\n",
    "    \n",
    "df_annual_trade_reports_1927 = pd.read_csv(annual_trade_reports_1927,index_col=None, encoding=encoding, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8\n"
     ]
    }
   ],
   "source": [
    "with open(annual_trade_reports_1928, 'rb') as f: \n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"detected encoding: {encoding}\")\n",
    "    \n",
    "df_annual_trade_reports_1928 = pd.read_csv(annual_trade_reports_1928,index_col=None, encoding=encoding, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: UTF-8-SIG\n"
     ]
    }
   ],
   "source": [
    "with open(returns_trade_ports, 'rb') as f: \n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"detected encoding: {encoding}\")\n",
    "    \n",
    "df_returns_trade_ports = pd.read_csv(returns_trade_ports,index_col=None, encoding=encoding, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: UTF-8-SIG\n"
     ]
    }
   ],
   "source": [
    "with open(shanghai_returns, 'rb') as f: \n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"detected encoding: {encoding}\")\n",
    "    \n",
    "df_shanghai_returns = pd.read_csv(shanghai_returns,index_col=None, encoding=encoding, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: UTF-8-SIG\n"
     ]
    }
   ],
   "source": [
    "with open(trade_statistics_treaty_ports, 'rb') as f: \n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"detected encoding: {encoding}\")\n",
    "    \n",
    "df_trade_statistics_treaty_ports = pd.read_csv(trade_statistics_treaty_ports,index_col=None, encoding=encoding, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      series_name  table_type  year    filepath_osn  \\\n",
      "0           Notes       Notes  1923    43675355.txt   \n",
      "1           Aigun       Notes  1923    43675356.txt   \n",
      "2           Aigun    Contents  1923    43675357.txt   \n",
      "3           Aigun       Notes  1923    43675358.txt   \n",
      "4           Aigun       Notes  1923    43675359.txt   \n",
      "...           ...         ...   ...             ...   \n",
      "16284    Tientsin  Population  1873    44319950.txt   \n",
      "16285    Tientsin  Population  1873  44319950_a.csv   \n",
      "16286    Tientsin  Population  1873  44319950_b.csv   \n",
      "16287    Tientsin  Population  1873    44319951.txt   \n",
      "16288    Tientsin  Population  1873    44319952.txt   \n",
      "\n",
      "                              custom_name    filename_osn file_type  \\\n",
      "0                            43675355.txt    43675355.txt       txt   \n",
      "1                            43675356.txt    43675356.txt       txt   \n",
      "2                            43675357.txt    43675357.txt       txt   \n",
      "3                            43675358.txt    43675358.txt       txt   \n",
      "4                            43675359.txt    43675359.txt       txt   \n",
      "...                                   ...             ...       ...   \n",
      "16284    Tientsin-Population_44319950.txt    44319950.txt       txt   \n",
      "16285  Tientsin-Population_44319950_a.csv  44319950_a.csv       csv   \n",
      "16286  Tientsin-Population_44319950_b.csv  44319950_b.csv       csv   \n",
      "16287    Tientsin-Population_44319951.txt    44319951.txt       txt   \n",
      "16288    Tientsin-Population_44319952.txt    44319952.txt       txt   \n",
      "\n",
      "               table_title                                           entities  \\\n",
      "0              Notes-Notes  December Quarter; China; one; The Annual Trade...   \n",
      "1              Aigun-Notes  Shanghai; Limited; HARVARD; LIBRARY; SERIES; A...   \n",
      "2           Aigun-Contents  the Maritime Customs; the Maritime Customs Tra...   \n",
      "3              Aigun-Notes  Blagovestchensk; the year; Russian; AIGUN; Cus...   \n",
      "4              Aigun-Notes  Aigun; Customs; the year; October; Chinese; Ru...   \n",
      "...                    ...                                                ...   \n",
      "16284  Tientsin-Population                                    Port; Customs,    \n",
      "16285  Tientsin-Population  Internal Communication - Land or Water; Seaboa...   \n",
      "16286  Tientsin-Population  Tientsin; CHINESE; FRENCH; NORWEGIAN; RUSSIAN;...   \n",
      "16287  Tientsin-Population                                                NaN   \n",
      "16288  Tientsin-Population                                                NaN   \n",
      "\n",
      "                                     author  ...  \\\n",
      "0      China. Hai guan zong shui wu si shu.  ...   \n",
      "1      China. Hai guan zong shui wu si shu.  ...   \n",
      "2      China. Hai guan zong shui wu si shu.  ...   \n",
      "3      China. Hai guan zong shui wu si shu.  ...   \n",
      "4      China. Hai guan zong shui wu si shu.  ...   \n",
      "...                                     ...  ...   \n",
      "16284   China. Hai guan zong shui wu si shu  ...   \n",
      "16285   China. Hai guan zong shui wu si shu  ...   \n",
      "16286   China. Hai guan zong shui wu si shu  ...   \n",
      "16287   China. Hai guan zong shui wu si shu  ...   \n",
      "16288   China. Hai guan zong shui wu si shu  ...   \n",
      "\n",
      "                                         published  \\\n",
      "0              China. Hai guan zong shui wu si shu   \n",
      "1              China. Hai guan zong shui wu si shu   \n",
      "2              China. Hai guan zong shui wu si shu   \n",
      "3              China. Hai guan zong shui wu si shu   \n",
      "4              China. Hai guan zong shui wu si shu   \n",
      "...                                            ...   \n",
      "16284  Shanghai : Imperial Maritime Customs, 1873.   \n",
      "16285  Shanghai : Imperial Maritime Customs, 1873.   \n",
      "16286  Shanghai : Imperial Maritime Customs, 1873.   \n",
      "16287  Shanghai : Imperial Maritime Customs, 1873.   \n",
      "16288  Shanghai : Imperial Maritime Customs, 1873.   \n",
      "\n",
      "                                                subjects       topic_class  \\\n",
      "0      China -- Commerce -- Statistics -- Periodicals...  Economic History   \n",
      "1      China -- Commerce -- Statistics -- Periodicals...  Economic History   \n",
      "2      China -- Commerce -- Statistics -- Periodicals...  Economic History   \n",
      "3      China -- Commerce -- Statistics -- Periodicals...  Economic History   \n",
      "4      China -- Commerce -- Statistics -- Periodicals...  Economic History   \n",
      "...                                                  ...               ...   \n",
      "16284  China -- Commerce -- Statistics -- Periodicals...  Economic History   \n",
      "16285  China -- Commerce -- Statistics -- Periodicals...  Economic History   \n",
      "16286  China -- Commerce -- Statistics -- Periodicals...  Economic History   \n",
      "16287  China -- Commerce -- Statistics -- Periodicals...  Economic History   \n",
      "16288  China -- Commerce -- Statistics -- Periodicals...  Economic History   \n",
      "\n",
      "      creation_date             record_id  \\\n",
      "0              1923  990058000000000000.0   \n",
      "1              1923  990058000000000000.0   \n",
      "2              1923  990058000000000000.0   \n",
      "3              1923  990058000000000000.0   \n",
      "4              1923  990058000000000000.0   \n",
      "...             ...                   ...   \n",
      "16284          1873    990058255570203941   \n",
      "16285          1873    990058255570203941   \n",
      "16286          1873    990058255570203941   \n",
      "16287          1873    990058255570203941   \n",
      "16288          1873    990058255570203941   \n",
      "\n",
      "                                               permalink  \\\n",
      "0      https://id.lib.harvard.edu/alma/99005825550020...   \n",
      "1      https://id.lib.harvard.edu/alma/99005825550020...   \n",
      "2      https://id.lib.harvard.edu/alma/99005825550020...   \n",
      "3      https://id.lib.harvard.edu/alma/99005825550020...   \n",
      "4      https://id.lib.harvard.edu/alma/99005825550020...   \n",
      "...                                                  ...   \n",
      "16284  https://id.lib.harvard.edu/alma/99005825557020...   \n",
      "16285  https://id.lib.harvard.edu/alma/99005825557020...   \n",
      "16286  https://id.lib.harvard.edu/alma/99005825557020...   \n",
      "16287  https://id.lib.harvard.edu/alma/99005825557020...   \n",
      "16288  https://id.lib.harvard.edu/alma/99005825557020...   \n",
      "\n",
      "                                                     url innodata_id  \\\n",
      "0      https://iiif.lib.harvard.edu/manifests/view/dr...         NaN   \n",
      "1      https://iiif.lib.harvard.edu/manifests/view/dr...         NaN   \n",
      "2      https://iiif.lib.harvard.edu/manifests/view/dr...         NaN   \n",
      "3      https://iiif.lib.harvard.edu/manifests/view/dr...         NaN   \n",
      "4      https://iiif.lib.harvard.edu/manifests/view/dr...         NaN   \n",
      "...                                                  ...         ...   \n",
      "16284  https://iiif.lib.harvard.edu/manifests/view/dr...         NaN   \n",
      "16285  https://iiif.lib.harvard.edu/manifests/view/dr...         NaN   \n",
      "16286  https://iiif.lib.harvard.edu/manifests/view/dr...         NaN   \n",
      "16287  https://iiif.lib.harvard.edu/manifests/view/dr...         NaN   \n",
      "16288  https://iiif.lib.harvard.edu/manifests/view/dr...         NaN   \n",
      "\n",
      "       Unnamed: 10  series_trim  \n",
      "0              NaN          NaN  \n",
      "1              NaN          NaN  \n",
      "2              NaN          NaN  \n",
      "3              NaN          NaN  \n",
      "4              NaN          NaN  \n",
      "...            ...          ...  \n",
      "16284          NaN          NaN  \n",
      "16285          NaN          NaN  \n",
      "16286          NaN          NaN  \n",
      "16287          NaN          NaN  \n",
      "16288          NaN          NaN  \n",
      "\n",
      "[16289 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "#concatenate dataframes\n",
    "dfs = [df_annual_trade_reports_1923,\n",
    "       df_annual_trade_reports_1924,\n",
    "       df_annual_trade_reports_1925,\n",
    "       df_annual_trade_reports_1926,\n",
    "       df_annual_trade_reports_1927,\n",
    "       df_annual_trade_reports_1928,\n",
    "       df_returns_trade_ports,\n",
    "       df_shanghai_returns, \n",
    "       df_trade_statistics_treaty_ports]\n",
    "\n",
    "result = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(base_dir+ \"chinese_maritime_customs_metadata_inventory.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Curation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Create a single dataset\n",
    "This test allows users to create a single dataset and upload its related datafiles. \n",
    "Useful for troubleshooting and to test other collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Test: Create datafile metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datafile metadata\n",
    "# get the first series\n",
    "first_series = g_series_names[1]\n",
    "first_series_metadata = g_dataset_metadata[first_series]\n",
    "first_series_inventory_df = g_series_inventories[first_series]\n",
    "\n",
    "# set the template\n",
    "template = 'File associated with data tables series:'\n",
    "datafile_metadata_df = curate.create_datafile_metadata(first_series_inventory_df, g_datafile_description_template_csv, g_datafile_description_template_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Test: Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test dataset\n",
    "dataset_ret = curate.create_dataset(g_api, g_dataverse_collection, first_series_metadata)\n",
    "pprint.pprint(dataset_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.25. Test: using easyDataverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyDataverse ### IMPORTANT ##### easyDataverse works with an earlier version of pydantic. DVUploader (python) works with a newer version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Test: test single file with python DVUploader \n",
    "Hard coded doi & file directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvuploader as dv\n",
    "\n",
    "files = [\n",
    "    #File(filepath=\"/Users/katherinemika/Desktop/test/43675367_a.csv\"),\n",
    "    #File(directoryLabel=\"txt\", filepath=\"/Users/katherinemika/Desktop/test/txt/43675367.txt\"),\n",
    "    *dv.add_directory(\"/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1925/test_batch/\"),\n",
    "]\n",
    "\n",
    "dvuploader = DVUploader(files=files)\n",
    "dvuploader.upload(\n",
    "    api_token=g_dataverse_api_key,\n",
    "    dataverse_url=\"https://dataverse.harvard.edu\",\n",
    "    persistent_id=\"doi:10.7910/DVN/WDBXNN\",\n",
    "    n_parallel_uploads= 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test: Direct upload the datafiles associated with the dataset (series name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# upload the series dataset datafiles \n",
    "#test_datafiles_path = \"/Users/katherinemika/Desktop/test/\"\n",
    "pid = dataset_ret.get('dataset_pid')\n",
    "ret = curate.direct_upload_datafiles(g_api, g_dataverse_installation_url, pid, g_datafiles_path, datafile_metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Test: Examine a directory to make certain all files exist before attempting an upload of datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see if all files are there and report the ones that aren't\n",
    "\n",
    "import os\n",
    "errors = {}\n",
    "for row in g_dataverse_inventory_df.iterrows():\n",
    "    filename = row[1].get('filename_osn')\n",
    "    filepath = g_datafiles_path + '/' + filename\n",
    "    if (os.path.exists(filepath)):\n",
    "        errors[filepath] = True\n",
    "    else:\n",
    "        print('File not found: {}'.format(filepath))\n",
    "        errors[filepath] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test and build functions for adding files to datasets via python dvuploader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "psuedo code: \n",
    "\n",
    "1. list of files to add to each dataset\n",
    "2. loop dvuploader.upload module for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvuploader as dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_dvuploader(api, dataverse_url, dataset_pid, data_directory, metadata_df):\n",
    "    \"\"\"\n",
    "    Upload Open Metadata datafiles to dataverse repository using direct upload method\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api : pyDataverse api\n",
    "    dataverse_url : str\n",
    "        Dataverse installation url (e.g., https://demo.dataverse.org)\n",
    "    dataset_pid : str\n",
    "        Persistent identifier for the dataset (its DOI, takes form: doi:xxxxx)\n",
    "    data_directory : str\n",
    "        Directory where datafiles are kept\n",
    "    metadata_df : DataFrame\n",
    "        DataFrame containing metadata about datafiles to upload\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    dict\n",
    "        {upload: bool, errors: list, finalize: bool}\n",
    "    \"\"\"\n",
    "\n",
    "    #validate params\n",
    "    if ((not api) or \n",
    "        (not dataverse_url) or\n",
    "        (not dataset_pid) or\n",
    "        (not data_directory) or\n",
    "        (metadata_df.empty==True)):\n",
    "        return False\n",
    "\n",
    "    #error msg\n",
    "    errors = []\n",
    "\n",
    "    json_data = []\n",
    "    cats = None\n",
    "\n",
    "    #add each file in metadata_df to files list for dvuploader\n",
    "    \n",
    "    files = []\n",
    "    \n",
    "    for row in metadata_df.iterrows():\n",
    "        file = row[1].get('filename_osn')\n",
    "        filepath = g_datafiles_path + \"/\" + file\n",
    "        file_name = row[1].get('custom_name')\n",
    "        desc = row[1].get('description')\n",
    "        mime_type = row[1].get('mimetype')\n",
    "\n",
    "        #format tags\n",
    "        tags = row[1].get('tags')\n",
    "        tags_lst = eval(tags)\n",
    "        \n",
    "        files.append(dv.File(filepath = filepath,\n",
    "                             file_name = file_name,\n",
    "                             description = desc,\n",
    "                             mimeType = mime_type,\n",
    "                             categories = tags_lst\n",
    "                            )\n",
    "                    )\n",
    "        \n",
    "        print('Uploading: {}/{} - {} {}'.format(data_directory, filepath, desc, mimeType))\n",
    "\n",
    "        \n",
    "    key = api.api_token\n",
    "    dvuploader = dv.DVUploader(files=files)\n",
    "        \n",
    "    dvuploader.upload(\n",
    "        api_token = key,\n",
    "        dataverse_url = dataverse_url,\n",
    "        persistent_id = dataset_pid,\n",
    "        n_parallel_uploads= 2 #however many your installation can handle\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = dataset_ret.get('dataset_pid')\n",
    "ret = python_dvuploader(g_api, g_dataverse_installation_url, pid, g_datafiles_path, datafile_metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Test: Delete all the datasets in the collection and start again\n",
    "- WARNING: This is a permanent operation. Be very certain you want to perform this operation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete all the datasets\n",
    "# ARE YOU SURE ABOUT THIS? if so, uncomment the next line and execute\n",
    "ret = curate.delete_datasets(g_api, g_dataverse_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End document.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e80866da39f614c41262712a96c603cec09e65c25ffba1b64ff6a9fa5a13fe2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
